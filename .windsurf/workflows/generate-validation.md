---
allowed-tools: "*"
description: Generate a project-specific validation command tailored to your codebase
---
allowed-tools: "*"

# Generate Validation Command

**One command to analyze your codebase and create a tailored validation workflow.**

> Run this once per project. It creates `.windsurf/commands/validate.md` customized to YOUR stack.

---
allowed-tools: "*"

## Step 0: Discover Real User Workflows
// turbo

**Before analyzing tooling, understand what users ACTUALLY do:**

1. **Read workflow documentation**:
   - `README.md` â€” "Usage", "Quickstart", "Examples" sections
   - `CLAUDE.md` / `AGENTS.md` â€” AI workflow patterns
   - `docs/` â€” User guides, tutorials, API docs

2. **Identify external integrations**:
   - What CLIs does the app use? (Check `Dockerfile`, scripts)
   - What external APIs? (Stripe, Twilio, GitHub, Slack, etc.)
   - What services does it interact with?

3. **Extract complete user journeys from docs**:
   - "User registers â†’ verifies email â†’ logs in â†’ creates item"
   - Each workflow becomes an E2E test scenario

**Output**: List of user workflows to validate.

---
allowed-tools: "*"

## Step 1: Deep Codebase Analysis
// turbo

### 1.1: Detect Validation Tooling

| Category | Look For | Command Pattern |
|---
allowed-tools: "*"-------|----------|-----------------|
| **Linting** | `.eslintrc*`, `ruff.toml`, `.pylintrc`, `golangci.yml` | `npm run lint`, `ruff check .`, `golangci-lint run` |
| **Type Checking** | `tsconfig.json`, `mypy.ini`, `pyproject.toml [mypy]` | `tsc --noEmit`, `mypy .`, `pyright` |
| **Style/Formatting** | `.prettierrc*`, `pyproject.toml [black]`, `.editorconfig` | `prettier --check .`, `black --check .` |
| **Unit Tests** | `jest.config.*`, `vitest.config.*`, `pytest.ini`, `*_test.go` | `npm test`, `pytest`, `go test ./...` |
| **E2E Tests** | `playwright.config.*`, `cypress.config.*`, `tests/e2e/` | `npx playwright test`, `npx cypress run` |
| **Package Scripts** | `package.json` scripts, `Makefile`, `pyproject.toml` | Check for existing `test`, `lint`, `validate` |

### 1.2: Detect Application Architecture

| Pattern | Indicators |
|---
allowed-tools: "*"------|------------|
| **Frontend** | `src/pages/`, `src/app/`, `src/routes/`, React/Vue/Svelte components |
| **Backend API** | `src/api/`, `routes/`, `controllers/`, FastAPI/Express/Gin routes |
| **Database** | `prisma/schema.prisma`, `drizzle/`, `migrations/`, `models/` |
| **Auth Provider** | Clerk, Stytch, NextAuth, Firebase, Supabase, Passport, custom |
| **Monorepo** | `pnpm-workspace.yaml`, `turbo.json`, `nx.json`, `lerna.json` |
| **Microservices** | Multiple `Dockerfile`s, `docker-compose.yml` with services |

### 1.3: Map All Entry Points

1. **API Routes**: List all endpoints (REST, GraphQL, tRPC, gRPC)
2. **Pages/Views**: List all frontend routes
3. **CLI Commands**: If app has CLI interface
4. **Webhooks**: Incoming webhook handlers
5. **Background Jobs**: Cron, queues, workers
6. **Database Schema**: Tables, relationships, constraints

### 1.4: Analyze Existing Tests

- What's already covered?
- What test patterns are used?
- What's the test directory structure?
- Any CI/CD configs? (`.github/workflows/`, `.gitlab-ci.yml`)

**Output**: Complete picture of stack, tooling, and coverage.

---
allowed-tools: "*"

## Step 2: Generate Project-Specific validate.md

Create `.windsurf/commands/validate.md` with the following structure:

```markdown
---
allowed-tools: "*"
description: Complete validation for [PROJECT_NAME] â€” if this passes, it works
---
allowed-tools: "*"

# Validate [PROJECT_NAME]

> Auto-generated by /generate-validation on [DATE]
> Tailored to: [DETECTED_STACK]

---
allowed-tools: "*"

## Phase 1: Static Analysis
// turbo

### 1.1: Linting
[DETECTED_LINT_COMMAND or skip if none]

### 1.2: Type Checking
[DETECTED_TYPE_CHECK_COMMAND or skip if none]

### 1.3: Style Checking
[DETECTED_FORMAT_CHECK_COMMAND or skip if none]

### 1.4: Security Audit
[DETECTED_AUDIT_COMMAND or skip if none]

**If any phase fails**: Stop and report errors.

---
allowed-tools: "*"

## Phase 2: Unit Tests
// turbo

[DETECTED_UNIT_TEST_COMMAND]

Expected coverage: [CURRENT_COVERAGE or "track after first run"]

---
allowed-tools: "*"

## Phase 3: Integration Tests
// turbo

[IF docker-compose EXISTS:]
```bash
docker compose up -d
timeout 60 bash -c 'until curl -sf http://localhost:[PORT]/health; do sleep 2; done'
[DETECTED_INTEGRATION_TEST_COMMAND]
```

[IF NO docker-compose:]
[DETECTED_INTEGRATION_TEST_COMMAND or "No integration tests configured"]

---
allowed-tools: "*"

## Phase 4: End-to-End Tests
// turbo

### 4.1: Setup Test Environment

[AUTH_SETUP based on detected provider:]
- Clerk: Load test instance credentials
- Stytch: Use test project
- NextAuth: Use test bypass or saved session
- Firebase: Use emulator
- Custom: [Detected pattern]

### 4.2: API Endpoint Coverage

Test EVERY endpoint discovered:

| Endpoint | Method | Auth Required | Test |
|---
allowed-tools: "*"-------|--------|---------------|------|
[LIST ALL DETECTED ENDPOINTS]

### 4.3: User Journey Tests

**From documentation, test these complete flows:**

[FOR EACH WORKFLOW FROM STEP 0:]
1. **[Workflow Name]**
   - Step: [action] â†’ Verify: [expected result]
   - Step: [action] â†’ Verify: [expected result]
   - Database check: [verification query]

### 4.4: External Integration Tests

[FOR EACH DETECTED INTEGRATION:]
- **[Service Name]**: [Test command or verification]

### 4.5: Database Integrity

[BASED ON DETECTED DATABASE:]
```bash
[DATABASE_SPECIFIC_VERIFICATION_COMMANDS]
```

Check for:
- Orphaned records
- Constraint violations
- Expected row counts
- Test data cleanup

---
allowed-tools: "*"

## Phase 5: Cleanup
// turbo

```bash
docker compose down -v 2>/dev/null || true
rm -rf coverage/ .nyc_output/ test-results/ playwright-report/
```

---
allowed-tools: "*"

## Phase 6: Report

```
ğŸ VALIDATION COMPLETE: [PROJECT_NAME]

ğŸ“Š STATUS: [âœ… PASSED / âŒ FAILED]

ğŸ“ˆ Results:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase           â”‚ Status â”‚ Details â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Linting         â”‚        â”‚         â”‚
â”‚ Type Checking   â”‚        â”‚         â”‚
â”‚ Style Check     â”‚        â”‚         â”‚
â”‚ Security Audit  â”‚        â”‚         â”‚
â”‚ Unit Tests      â”‚        â”‚         â”‚
â”‚ Integration     â”‚        â”‚         â”‚
â”‚ E2E: APIs       â”‚        â”‚         â”‚
â”‚ E2E: Journeys   â”‚        â”‚         â”‚
â”‚ E2E: External   â”‚        â”‚         â”‚
â”‚ Database        â”‚        â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[If PASSED:] âœ… Ready for deployment. Manual testing unnecessary.
[If FAILED:] âŒ Fix issues and run /validate again.
```
```

---
allowed-tools: "*"

## Step 3: Validate the Generated Command
// turbo

1. **Read the generated file** to verify it's correct
2. **Check for placeholders** â€” all `[BRACKETS]` should be replaced with actual values
3. **Verify commands exist** â€” don't include commands that don't work
4. **Test a dry run** if possible

---
allowed-tools: "*"

## Step 4: Report

```
âœ… VALIDATION COMMAND GENERATED

ğŸ“„ Created: .windsurf/commands/validate.md

ğŸ” Detected Stack:
   - Language: [X]
   - Framework: [X]
   - Database: [X]
   - Auth: [X]
   - Test Runner: [X]

ğŸ“‹ Validation Phases:
   1. Static Analysis: [tools detected]
   2. Unit Tests: [command]
   3. Integration: [command]
   4. E2E: [N] endpoints, [N] user journeys, [N] integrations
   5. Database: [verification method]

ğŸš€ Run validation:
   /validate

ğŸ’¡ Re-generate after major stack changes:
   /generate-validation
```

---
allowed-tools: "*"

## Critical Reminders

**The generated validate.md should:**
- âœ… Use actual commands from the project (not generic examples)
- âœ… Test every API endpoint discovered
- âœ… Cover every user workflow from docs
- âœ… Exercise every external integration
- âœ… Verify database integrity
- âœ… Be immediately executable without modification

**Don't generate placeholders** â€” if you can't detect something, omit that section or mark it as "Not configured".

**The goal**: Run `/validate` and have 100% confidence the app works.
